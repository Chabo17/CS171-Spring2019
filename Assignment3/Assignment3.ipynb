{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "### Instructor: Vagelis Papalexakis\n",
    "### Credit for  Assignment 3: 10/35 points of the final grade\n",
    "\n",
    "In this assignment we will implement the K-means clustering algorithm. We are going to use the same dataset as in the previous two assignments (<b>Note</b>: make sure you copy the dataset from Assignment 1 to the folder of this assignment!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/ericong18/Library/Python/3.7/lib/python/site-packages (1.16.2)\n",
      "Requirement already satisfied: pandas in /Users/ericong18/Library/Python/3.7/lib/python/site-packages (0.24.2)\n",
      "Requirement already satisfied: matplotlib in /Users/ericong18/Library/Python/3.7/lib/python/site-packages (3.0.3)\n",
      "Requirement already satisfied: seaborn in /Users/ericong18/Library/Python/3.7/lib/python/site-packages (0.9.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/ericong18/Library/Python/3.7/lib/python/site-packages (0.20.3)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/ericong18/Library/Python/3.7/lib/python/site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.7/site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/ericong18/Library/Python/3.7/lib/python/site-packages (from matplotlib) (2.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ericong18/Library/Python/3.7/lib/python/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ericong18/Library/Python/3.7/lib/python/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /Users/ericong18/Library/Python/3.7/lib/python/site-packages (from seaborn) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (40.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user numpy pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import random as rand\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'label']\n",
    "data = pd.read_csv('iris.data', \n",
    "                   names = data_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Implementing and testing K-means clustering [100%]\n",
    "### Question 1a: Implementing K-Means clustering [50%]\n",
    "In this question you should implement a function that performs k-means clustering, using the Euclidean distance (you may use Numpy libraries for the distance computation). For calculation of the centroid you should use the 'mean' function.\n",
    "\n",
    "For uniformity, you should implement a function with the following specifications:\n",
    "```python\n",
    "def kmeans_clustering(all_vals,K,max_iter = 100, tol = pow(10,-3) ):\n",
    "```\n",
    "where 1) 'all_vals' is the $N \\times M$ matrix that contains all data points ($N$ is the number of data points and $M$ is the number of features, each row of the matrix is a data point), 2) 'K' is the number of clusters, 3) 'max_iter' is the maxium number of iterations, and 4) 'tol' is the tolerance for the change of the sum of squares of errors that determines convergence.\n",
    "\n",
    "Your function should return the following variables: 1) 'assignments': this is a $N\\times 1$ vector (where $N$ is the number of data points) where the $i$-th position of that vector contains the cluster number that the $i$-th data point is assigned to, 2) 'centroids': this is a $K\\times M$ matrix, each row of which contains the centroid for every cluster, 3) 'all_sse': this is a vector that contains all the sum of squares of errors per iteration of the algorithm, and 4) 'iters': this is the number of iterations that the algorithm ran.\n",
    "\n",
    "Here we are going to implement the simplest version of K-means, where the initial centroids are chosen entirely at random among all the data points.\n",
    "\n",
    "As we saw in class, the K-means algorithm iterates over the following steps:\n",
    "- Given a set of centroids, assign all data points to the cluster represented by its nearest centroid (according to Euclidean distance)\n",
    "- Given a set of assignments of points to clusters, compute the new centroids for every cluster, by taking the mean of all the points assigned to each cluster.\n",
    "\n",
    "Your algorithm should converge if 1) the maximum number of iteratiosn is reached, or 2) if the SSE between two consecutive iterations does not change a lot (as in the gradient descent for linear regression we saw in Assignment 2). In order to check for the latter condition, you may use the following piece of code:\n",
    "```python\n",
    "if np.absolute(all_sse[it] - all_sse[it-1])/all_sse[it-1] <= tol\n",
    "```\n",
    "\n",
    "In order to calculate the SSE (sum of squares of error) first you need to define what an 'error' is. In k-means, error per data point refers to the Euclidean distance of that particular point from its assigned centroid. SSE sums up all those squared Euclidean distances for all data points and comes up with a number that reflects the total error of approximating every data points by its assigned centroid.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2], [[6.853846153846153, 3.0769230769230766, 5.715384615384615, 2.053846153846153], [5.005999999999999, 3.4180000000000006, 1.464, 0.2439999999999999], [5.88360655737705, 2.740983606557377, 4.388524590163935, 1.4344262295081966]], [298.85999999999996, 146.0426377445022, 141.31679606263205, 97.97737196429983, 79.17030348365, 78.94506582597724, 78.94506582597724], 6) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k-means clustering\n",
    "\n",
    "def sum_of_squares_of_errors(all_vals, assignments, centroids):\n",
    "    errors = []\n",
    "    \n",
    "    for i, data_point in enumerate(all_vals):\n",
    "        assigned_centroid = centroids[assignments[i]]\n",
    "        error = np.linalg.norm(np.array(data_point) - np.array(assigned_centroid))\n",
    "        errors.append(error**2)\n",
    "        \n",
    "    return sum(errors)\n",
    "\n",
    "def kmeans_clustering(all_vals, K, max_iter = 100, tol = pow(10,-3)):\n",
    "    iters = 0\n",
    "    assignments = []\n",
    "    all_sse = []\n",
    "    \n",
    "    # Select K random, unique centroids from dataset\n",
    "    centroids = rand.sample(all_vals, k=K)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        assignments.clear()\n",
    "        \n",
    "        # Initialize groupings for each cluster\n",
    "        centroid_groups = [None] * K\n",
    "        for j in range(len(centroid_groups)):\n",
    "            centroid_groups[j] = []\n",
    "        \n",
    "        for data_point in all_vals:\n",
    "            min_distance = float('inf')\n",
    "            min_cluster_index = 0\n",
    "            \n",
    "            # Find the closest centroid for each data point\n",
    "            for index, centroid in enumerate(centroids):\n",
    "                # Calculate the euclidean distance\n",
    "                distance = np.linalg.norm(np.array(data_point) - np.array(centroid))\n",
    "                \n",
    "                # Calculate the closest cluster\n",
    "                if distance < min_distance:\n",
    "                    min_cluster_index = index\n",
    "                    min_distance = distance\n",
    "            \n",
    "            # Append the closest cluster to the assignments list\n",
    "            assignments.append(min_cluster_index)\n",
    "            centroid_groups[min_cluster_index].append(data_point)\n",
    "            \n",
    "        # Calculate sum of squares of errors\n",
    "        sse = sum_of_squares_of_errors(all_vals, assignments, centroids)\n",
    "        all_sse.append(sse)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if i > 0 and np.absolute(all_sse[i] - all_sse[i-1])/all_sse[i-1] <= tol:\n",
    "            break\n",
    "        \n",
    "        # Calculate new centroids\n",
    "        for index, val in enumerate(centroid_groups):\n",
    "            mean_centroid = np.mean(val, axis=0).tolist()\n",
    "            centroids[index] = mean_centroid\n",
    "        \n",
    "        # Count the number of iterations\n",
    "        iters += 1\n",
    "        \n",
    "    return (assignments, centroids, all_sse, iters)\n",
    "    \n",
    "all_vals = (data[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values).tolist()\n",
    "kmc_result = kmeans_clustering(all_vals, 3)\n",
    "print(kmc_result, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1b: Visualizing K-means [10%]\n",
    "In this question we wll visualize the result of the K-means algorithm. For ease of visualization, we will focus on a scatterplot of two of the four features of the Iris dataset. In particular: run your K-means code with K=3 and default values for the rest of the inputs. Subsequently, make a single scatterplot that contains all data points of the dataset for features 'sepal_length' and 'petal_length' and color every data point according to its cluster assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1c: Testing K-means [40%]\n",
    "Selecting the right number of clusters $K$ is a very challenging problem, especially when we don't have some side-information or domain expertise that can help us narrow down a few reasonable values for that parameter. \n",
    "\n",
    "In the absence of any other information, a very useful exercise is to create the plot of SSE (sum of squares of errors) as a function of $K$. Ideally, for a very small $K$, the error will be high (since we are trying to approximate a whole lot of points with a very small number of centroids) and as $K$ increases, the error decreases. However, after a certain value (or a couple of values) for $K$, we will notice diminishing returns, i.e., the error will be decreasing, but not to a great degree. Typically, the value(s) for $K$ where this behavior is observed (the threshold point after which we observe diminishing returns) is usually a good guess for the number of clusters. \n",
    "\n",
    "In this question, we will have to create the SSE vs. K plot for $K = 1\\cdots10$. Furthermore, because K-means uses randomized initialization, we need to do a number of iterations per value of $K$ in order to get a good estimate of the actual SSE (which may not be caused by randomness in the initialization). For this question, you will have to run the entire K-means algorithm to completion, and repeat it 50 different times per $K$, and collect all SSEs. In the figure, you should report the mean SSE per $K$, surrounded by error-bars which will encode the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
